{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Template\n",
    "This is the template for the first homework assignment.\n",
    "Below are some function templates which we require you to fill out.\n",
    "These will be tested by the autograder, so it is important to not edit the function definitions.\n",
    "The functions have python docstrings which should indicate what the input and output arguments are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the Autograder\n",
    "When you submit your code to the autograder on Gradescope, you will need to comment out any code which is not an import statement or contained within a function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this code if you want to verify your `sklearn` installation.\n",
    "# If this cell outputs 'array([1])', then it's installed correctly.\n",
    "\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code to see how to visualize a decision tree. This code should\n",
    "# be commented out when you submit to the autograder.\n",
    "# If this cell fails with\n",
    "# an error related to `pydotplus`, try running `pip install pydotplus`\n",
    "# from the command line, and retry. Similarly for any other package failure message.\n",
    "# If you can't get this cell working, it's ok - this part is not required.\n",
    "#\n",
    "# This part should be commented out when you submit it to Gradescope\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file=dot_data,  \n",
    "               filled=True, rounded=True,\n",
    "               special_characters=True,\n",
    "               feature_names=['feature1', 'feature2'],\n",
    "               class_names=['0', '1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code should be commented out when you submit to the autograder.\n",
    "# This cell will possibly download and unzip the dataset required for this assignment.\n",
    "# It hasn't been tested on Windows, so it will not run if you are running on Windows.\n",
    "\n",
    "import os\n",
    "\n",
    "if os.name != 'nt':  # This is the Windows check\n",
    "   if not os.path.exists('badges.zip'):\n",
    "       # If your statement starts with \"!\", then the command is run in bash, not python\n",
    "       !wget https://www.seas.upenn.edu/~cis519/fall2018/assets/HW/HW1/badges.zip\n",
    "       !mkdir -p badges\n",
    "       !unzip badges.zip -d badges\n",
    "       print('The data has saved in the \"badges\" directory.')\n",
    "else:\n",
    "   print('Sorry, I think you are running on windows. '\n",
    "         'You will need to manually download the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_train_db(train_file):\n",
    "    \"\"\"\n",
    "    Makes a list from the train txt file.\n",
    "    \"\"\"\n",
    "    f = open(train_file)\n",
    "    train_info = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        rows = line.split('\\t')\n",
    "        train_info.append(rows)\n",
    "    f.close()\n",
    "            \n",
    "    return train_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train list using txt file\n",
    "unlabeled_list = create_train_db('./badges/test.unlabeled.txt')\n",
    "train_list = create_train_db('./badges/train.txt')\n",
    "train0_list = create_train_db('./badges/train.fold-0.txt')\n",
    "train1_list = create_train_db('./badges/train.fold-1.txt')\n",
    "train2_list = create_train_db('./badges/train.fold-2.txt')\n",
    "train3_list = create_train_db('./badges/train.fold-3.txt')\n",
    "train4_list = create_train_db('./badges/train.fold-4.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_features(name):\n",
    "    \"\"\"\n",
    "    Compute all of the features for a given name. The input\n",
    "    name will always have 3 names separated by a space.\n",
    "    \n",
    "    Args:\n",
    "        name (str): The input name, like \"bill henry gates\".\n",
    "    Returns:\n",
    "        list: The features for the name as a list, like [0, 0, 1, 0, 1].\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    names = name.split()\n",
    "    for j in range(3):\n",
    "        name_list = []\n",
    "        for i in range(5):\n",
    "            char_list = [0]*26\n",
    "            if(i < len(names[j])):\n",
    "                if names[j][i] != '-':\n",
    "                    char_list[ord(names[j][i])-97] = 1\n",
    "            name_list += char_list\n",
    "        features_list += name_list\n",
    "    return features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unlabled_X (lst):\n",
    "    X = []\n",
    "    for row in lst:\n",
    "        X.append(compute_features(row[0]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X (lst):\n",
    "    X = []\n",
    "    for row in lst:\n",
    "        X.append(compute_features(row[1]))\n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features X from train_fold \n",
    "X_unlabeled = create_unlabled_X(unlabeled_list)\n",
    "X_all = create_X(train_list)\n",
    "X0 = create_X(train0_list)\n",
    "X1 = create_X(train1_list)\n",
    "X2 = create_X(train2_list)\n",
    "X3 = create_X(train3_list)\n",
    "X4 = create_X(train4_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y(lst):\n",
    "    y = []\n",
    "    for row in lst:\n",
    "        y.append(1 if row[0] == '+' else 0)\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create lables y from train_fold\n",
    "y_all =  create_y(train_list)\n",
    "y0 = create_y(train0_list)\n",
    "y1 = create_y(train1_list)  \n",
    "y2 = create_y(train2_list)\n",
    "y3 = create_y(train3_list)\n",
    "y4 = create_y(train4_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# The `max_depth=None` construction is how you specify default arguments\n",
    "# in python. By adding a default argument, you can call this method in a couple of ways:\n",
    "#     \n",
    "#     train_decision_tree(X, y)\n",
    "#     train_decision_tree(X, y, 4) or train_decision_tree(X, y, max_depth=4)\n",
    "#\n",
    "# In the first way, max_depth is automatically set to `None`, otherwise it is 4.\n",
    "def train_decision_tree(X, y, max_depth=None):\n",
    "    \"\"\"\n",
    "    Trains a decision tree on the input data using the information gain criterion\n",
    "    (set the criterion in the constructor to 'entropy').\n",
    "    \n",
    "    Args:\n",
    "        X (list of lists): The features, which is a list of length n, and\n",
    "                           each item in the list is a list of length d. This\n",
    "                           represents the n x d feature matrix.\n",
    "        y (list): The n labels, one for each item in X.\n",
    "        max_depth (int): The maximum depth the decision tree is allowed to be. If\n",
    "                         `None`, then the depth is unbounded.\n",
    "    Returns:\n",
    "        DecisionTreeClassifier: the learned decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=max_depth,criterion='entropy')\n",
    "    dt = dt.fit(X, y)\n",
    "    return dt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein (s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    memo = np.reros((len(s1)+1, len(s2)+1), np.init8)\n",
    "    \n",
    "    memo[0] = range(len(s2) + 1)\n",
    "    \n",
    "    # We are computing the next cell in (i+1, j+1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        memo[i+1, 0] = i + 1\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = memo[i, j+1] + 1\n",
    "            deletions = memo[i+1, j] + 1\n",
    "            substitutions = memo[i,j] + (c1!=c2)\n",
    "            memo[i+1, j+1] = min (insertions, deletions, substituions)\n",
    "    print(memo)\n",
    "    \n",
    "    return momo[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ouput_predictions( y, output_file_name):\n",
    "    output = []\n",
    "    for i in range(len(unlabeled_list)):\n",
    "        sign = '+' if y[i] == 1 else '-' \n",
    "        tmp = sign + '\\t' + unlabeled_list[i][0]+'\\n'\n",
    "        output.append(tmp)\n",
    "  \n",
    "    # To txt file\n",
    "    with open(output_file_name, 'w') as f:\n",
    "        for item in output:\n",
    "            f.write(item)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree with depth 4 and visualize it.\n",
    "\n",
    "dt0_dep4 = train_decision_tree(X1+X2+X3+X4, y1+y2+y3+y4, 4)\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(dt0_dep4, out_file=dot_data,filled=True, rounded=True,special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "def train_sgd(X, y, learning_rate='optimal'):\n",
    "    \"\"\"\n",
    "    Trains an `SGDClassifier` using 'log' loss on the input data.\n",
    "    \n",
    "    Args:\n",
    "        X (list of lists): The features, which is a list of length n, and\n",
    "                           each item in the list is a list of length d. This\n",
    "                           represents the n x d feature matrix.\n",
    "        y (list): The n labels, one for each item in X.\n",
    "        learning_rate (str): The learning rate to use. See http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    Returns:\n",
    "        SGDClassifier: the learned classifier.\n",
    "    \"\"\"\n",
    "    clf = SGDClassifier(loss='log',alpha = 0.001, tol=0.2,learning_rate='optimal', penalty='l2').fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6485714285714286\n"
     ]
    }
   ],
   "source": [
    "# get pA for the SGDClassifier(Cross-validation):\n",
    "\n",
    "def get_pA_for_sgd(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4):\n",
    "    clf0 = train_sgd(X1+X2+X3+X4, y1+y2+y3+y4)\n",
    "    clf1 = train_sgd(X0+X2+X3+X4, y0+y2+y3+y4)\n",
    "    clf2 = train_sgd(X0+X1+X3+X4, y0+y1+y3+y4)\n",
    "    clf3 = train_sgd(X0+X1+X2+X4, y0+y1+y2+y4)\n",
    "    clf4 = train_sgd(X0+X1+X2+X3, y0+y1+y2+y3)\n",
    "    pAs = [clf0.score(X0, y0),clf1.score(X1, y1),clf2.score(X2, y2),clf3.score(X3, y3),clf4.score(X4, y4)]\n",
    "    \n",
    "    return sum(pAs)/len(pAs), pAs\n",
    "\n",
    "# evaluate SGD\n",
    "pA_tuples_sgd = get_pA_for_sgd(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4)\n",
    "pA_sgd = pA_tuples_sgd[0]\n",
    "pAs_sgd = pA_tuples_sgd[1]\n",
    "print(pA_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8699999999999999\n"
     ]
    }
   ],
   "source": [
    "# get trA for the SGDClassifier:\n",
    "\n",
    "def get_trA_for_sgd(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4):\n",
    "    clf0 = train_sgd(X1+X2+X3+X4, y1+y2+y3+y4)\n",
    "    clf1 = train_sgd(X0+X2+X3+X4, y0+y2+y3+y4)\n",
    "    clf2 = train_sgd(X0+X1+X3+X4, y0+y1+y3+y4)\n",
    "    clf3 = train_sgd(X0+X1+X2+X4, y0+y1+y2+y4)\n",
    "    clf4 = train_sgd(X0+X1+X2+X3, y0+y1+y2+y3)\n",
    "    return (clf0.score(X1+X2+X3+X4, y1+y2+y3+y4)+clf1.score(X0+X2+X3+X4, y0+y2+y3+y4)+clf2.score(X0+X1+X3+X4, y0+y1+y3+y4)+\n",
    "           clf3.score(X0+X1+X2+X4, y0+y1+y2+y4)+clf4.score(X0+X1+X2+X3, y0+y1+y2+y3))/5\n",
    "\n",
    "# # evaluate SGD\n",
    "trA_sgd = get_trA_for_sgd(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4)\n",
    "print(trA_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5742857142857142 0.5985714285714285 0.5642857142857143\n"
     ]
    }
   ],
   "source": [
    "# get pA for the decision_tree:\n",
    "\n",
    "def get_pA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=None):\n",
    "    dt0 = train_decision_tree(X1+X2+X3+X4, y1+y2+y3+y4,maxDepth)\n",
    "    dt1 = train_decision_tree(X0+X2+X3+X4, y0+y2+y3+y4,maxDepth)\n",
    "    dt2 = train_decision_tree(X0+X1+X3+X4, y0+y1+y3+y4,maxDepth)\n",
    "    dt3 = train_decision_tree(X0+X1+X2+X4, y0+y1+y2+y4,maxDepth)\n",
    "    dt4 = train_decision_tree(X0+X1+X2+X3, y0+y1+y2+y3,maxDepth)\n",
    "    \n",
    "    pAs = []\n",
    "    pAs.append(dt0.score(X0, y0))\n",
    "    pAs.append(dt1.score(X1, y1))\n",
    "    pAs.append(dt2.score(X2, y2))\n",
    "    pAs.append(dt3.score(X3, y3))\n",
    "    pAs.append(dt4.score(X4, y4))\n",
    "    return sum(pAs)/len(pAs),pAs\n",
    "\n",
    "#evaluate decision tree with maxDepth = None, 4, 8\n",
    "\n",
    "pAs_tuple_dt = get_pA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=None)\n",
    "pAs_tuple_dt4 = get_pA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=4)\n",
    "pAs_tuple_dt8 = get_pA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=8)\n",
    "\n",
    "pA_for_dt = pAs_tuple_dt[0]\n",
    "pA_for_dt4 = pAs_tuple_dt4[0]\n",
    "pA_for_dt8 = pAs_tuple_dt8[0]\n",
    "\n",
    "pAs_for_dt = pAs_tuple_dt[1]\n",
    "pAs_for_dt4 = pAs_tuple_dt4[1]\n",
    "pAs_for_dt8 = pAs_tuple_dt8[1]\n",
    "\n",
    "print(pA_for_dt,pA_for_dt4,pA_for_dt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.6639285714285714 0.7414285714285714\n"
     ]
    }
   ],
   "source": [
    "# get trA for the decision_tree:\n",
    "\n",
    "def get_trA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=None):\n",
    "    dt0 = train_decision_tree(X1+X2+X3+X4, y1+y2+y3+y4,maxDepth)\n",
    "    dt1 = train_decision_tree(X0+X2+X3+X4, y0+y2+y3+y4,maxDepth)\n",
    "    dt2 = train_decision_tree(X0+X1+X3+X4, y0+y1+y3+y4,maxDepth)\n",
    "    dt3 = train_decision_tree(X0+X1+X2+X4, y0+y1+y2+y4,maxDepth)\n",
    "    dt4 = train_decision_tree(X0+X1+X2+X3, y0+y1+y2+y3,maxDepth)\n",
    "    \n",
    "    scores = []\n",
    "    scores.append(dt0.score(X1+X2+X3+X4, y1+y2+y3+y4))\n",
    "    scores.append(dt1.score(X0+X2+X3+X4, y0+y2+y3+y4))\n",
    "    scores.append(dt2.score(X0+X1+X3+X4, y0+y1+y3+y4))\n",
    "    scores.append(dt3.score(X0+X1+X2+X4, y0+y1+y2+y4))\n",
    "    scores.append(dt4.score(X0+X1+X2+X3, y0+y1+y2+y3))\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "# get trA for the decision tree with maxDepth = None, 4, 8\n",
    "trA_for_dt = get_trA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=None)\n",
    "trA_for_dt4 = get_trA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=4)\n",
    "trA_for_dt8 = get_trA_for_decision_tree(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4, maxDepth=8)\n",
    "print(trA_for_dt,trA_for_dt4,trA_for_dt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_sgd_with_stumps(X, y):\n",
    "    \"\"\"\n",
    "    Trains an `SGDClassifier` using 'log' loss on the input data. The classifier will\n",
    "    be trained on features that are computed using decision tree stumps.\n",
    "    \n",
    "    This function will return two items, the `SGDClassifier` and list of `DecisionTreeClassifier`s\n",
    "    which were used to compute the new feature set. If `sgd` is the name of your `SGDClassifier`\n",
    "    and `stumps` is the name of your list of `DecisionTreeClassifier`s, then writing\n",
    "    `return sgd, stumps` will return both of them at the same time.\n",
    "    \n",
    "    Args:\n",
    "        X (list of lists): The features, which is a list of length n, and\n",
    "                           each item in the list is a list of length d. This\n",
    "                           represents the n x d feature matrix.\n",
    "        y (list): The n labels, one for each item in X.\n",
    "    Returns:\n",
    "        SGDClassifier: the learned classifier.\n",
    "        List[DecisionTree]: the decision stumps that were used to compute the features\n",
    "                            for the `SGDClassifier`.\n",
    "    \"\"\"\n",
    "    # This is an example for how to return multiple arguments\n",
    "    # in python. If you write `a, b = train_sgd_with_stumps(X, y)`, then\n",
    "    # a will be 1 and b will be 2.\n",
    "    \n",
    "    # Get 200 different decition stumps\n",
    "    stumps = []\n",
    "    for i in range(200):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size = 0.5)\n",
    "        stump =train_decision_tree(X_train, y_train, 8)\n",
    "        stumps.append(stump)\n",
    "    \n",
    "    # Predictions of 200 stumps\n",
    "    predictions = []\n",
    "    for stump in stumps:\n",
    "        predictions.append(stump.predict(X))\n",
    "    \n",
    "    # Create new X using predictions above\n",
    "    X_new = []\n",
    "    for i in range(len(X)):\n",
    "        X_new.append([])\n",
    "        for prediction in predictions:\n",
    "            X_new[i].append(prediction[i])\n",
    " \n",
    "    # Train new SGDClassifier using new X    \n",
    "    sgd_with_stumps = train_sgd( X_new, y, learning_rate='optimal')  \n",
    "        \n",
    "    return sgd_with_stumps, stumps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clf0 = train_sgd_with_stumps(X1+X2+X3+X4, y1+y2+y3+y4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accurancy(y_predict, y_actual):\n",
    "    cnt = 0\n",
    "    for i in range(len(y_predict)):\n",
    "        y_p = y_predict[i]\n",
    "        y_a = y_actual[i]\n",
    "        if y_p == y_a:\n",
    "            cnt += 1\n",
    "    return cnt/len(y_predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SGD-DT classifier can't use the same function as the SGD or decision trees\n",
    "# because it requires an extra argument\n",
    "\n",
    "def predict_sgd_with_stumps(sgd, stumps, X):\n",
    "    \"\"\"\n",
    "    Predicts labels for all instances `X` using the `SGDClassifier` trained with\n",
    "    features computed from decision stumps. The input `X` will be a matrix of the\n",
    "    original features. The stumps will be used to map `X` from the original features\n",
    "    to the features that the `SGDClassifier` were trained with.\n",
    "    \n",
    "    Args:\n",
    "        sgd (`SGDClassifier`): the classifier that was trained with features computed\n",
    "                               using the input stumps.\n",
    "        stumps (List[DecisionTreeClassifier]): a list of `DecisionTreeClassifier`s that\n",
    "                                               were used to train the `SGDClassifier`.\n",
    "        X (list of lists): The features that were used to train the stumps (i.e. the original\n",
    "                           feature set).\n",
    "    Returns:\n",
    "        List[int]: the predicted labels for each instance in `X`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predictions of given stumps\n",
    "    predictions = []\n",
    "    for stump in stumps:\n",
    "        predictions.append(stump.predict(X))\n",
    "    \n",
    "    # Create new X using predictions above\n",
    "    X_new = []\n",
    "    for i in range(len(X)):\n",
    "        X_new.append([])\n",
    "        for prediction in predictions:\n",
    "            X_new[i].append(prediction[i])\n",
    "            \n",
    "    return predict_clf(sgd, X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to this function can be an `SGDClassifier` or a `DecisionTreeClassifier`.\n",
    "# Because they both use the same interface for predicting labels, \n",
    "# the code can be the same\n",
    "# for both of them.\n",
    "\n",
    "def predict_clf(clf, X):\n",
    "    \"\"\"\n",
    "    Predicts labels for all instances in `X` using the `clf` classifier. This function\n",
    "    will be the same for `DecisionTreeClassifier`s and `SGDClassifier`s.\n",
    "    \n",
    "    Args:\n",
    "        clf: (`SGDClassifier` or `DecisionTreeClassifier`): the trained classifier.\n",
    "        X (list of lists): The features, which is a list of length n, and\n",
    "                           each item in the list is a list of length d. This\n",
    "                           represents the n x d feature matrix.\n",
    "    Returns:\n",
    "        List[int]: the predicted labels for each instance in `X`.\n",
    "    \"\"\"\n",
    "    \n",
    "    return clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6557142857142857\n"
     ]
    }
   ],
   "source": [
    "# get pA for SGD_with_stumps_Classifier:\n",
    "\n",
    "def get_pA_for_sgd_with_stumps(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4):\n",
    "    res0 = train_sgd_with_stumps(X1+X2+X3+X4, y1+y2+y3+y4)\n",
    "    y_p0 = predict_sgd_with_stumps(res0[0], res0[1], X0)\n",
    "    pA0 = get_accurancy(y_p0, y0)\n",
    "    \n",
    "    \n",
    "    res1 = train_sgd_with_stumps(X0+X2+X3+X4, y0+y2+y3+y4)\n",
    "    y_p1 = predict_sgd_with_stumps(res1[0], res1[1], X1)\n",
    "    pA1 = get_accurancy(y_p1, y1)\n",
    "    \n",
    "    res2 = train_sgd_with_stumps(X0+X1+X3+X4, y0+y1+y3+y4)\n",
    "    y_p2 = predict_sgd_with_stumps(res2[0], res2[1], X2)\n",
    "    pA2 = get_accurancy(y_p2, y2)\n",
    "    \n",
    "    res3 = train_sgd_with_stumps(X0+X1+X2+X4, y0+y1+y2+y4)\n",
    "    y_p3 = predict_sgd_with_stumps(res3[0], res3[1], X3)\n",
    "    pA3 = get_accurancy(y_p3, y3)\n",
    "    \n",
    "    res4 = train_sgd_with_stumps(X0+X1+X2+X3, y0+y1+y2+y3)\n",
    "    y_p4 = predict_sgd_with_stumps(res4[0], res4[1], X4)\n",
    "    pA4 = get_accurancy(y_p4, y4)\n",
    "        \n",
    "    pAs = [pA0,pA1,pA2,pA3,pA4]\n",
    "    return sum(pAs)/5,pAs\n",
    "\n",
    "# get pA for SGD_with_stumps_Classifier\n",
    "pA_tuples = get_pA_for_sgd_with_stumps(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4)\n",
    "pA_for_sgd_with_stumps = pA_tuples[0]\n",
    "pAs_sgd_with_stumps = pA_tuples[1]\n",
    "print(pA_for_sgd_with_stumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9657142857142856\n"
     ]
    }
   ],
   "source": [
    "# get trA for SGD_with_stumps_Classifier:\n",
    "\n",
    "def get_trA_for_sgd_with_stumps(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4):\n",
    "    res0 = train_sgd_with_stumps(X1+X2+X3+X4, y1+y2+y3+y4)\n",
    "    y_p0 = predict_sgd_with_stumps(res0[0], res0[1], X1+X2+X3+X4)\n",
    "    trA0 = get_accurancy(y_p0, y1+y2+y3+y4)\n",
    "    \n",
    "    \n",
    "    res1 = train_sgd_with_stumps(X0+X2+X3+X4, y0+y2+y3+y4)\n",
    "    y_p1 = predict_sgd_with_stumps(res1[0], res1[1], X0+X2+X3+X4)\n",
    "    trA1 = get_accurancy(y_p1, y0+y2+y3+y4)\n",
    "    \n",
    "    res2 = train_sgd_with_stumps(X0+X1+X3+X4, y0+y1+y3+y4)\n",
    "    y_p2 = predict_sgd_with_stumps(res2[0], res2[1], X0+X1+X3+X4)\n",
    "    trA2 = get_accurancy(y_p2, y0+y1+y3+y4)\n",
    "    \n",
    "    res3 = train_sgd_with_stumps(X0+X1+X2+X4, y0+y1+y2+y4)\n",
    "    y_p3 = predict_sgd_with_stumps(res3[0], res3[1],X0+X1+X2+X4)\n",
    "    trA3 = get_accurancy(y_p3, y0+y1+y2+y4)\n",
    "    \n",
    "    res4 = train_sgd_with_stumps(X0+X1+X2+X3, y0+y1+y2+y3)\n",
    "    y_p4 = predict_sgd_with_stumps(res4[0], res4[1], X0+X1+X2+X3)\n",
    "    trA4 = get_accurancy(y_p4, y0+y1+y2+y3)\n",
    "        \n",
    "    \n",
    "    return (trA0+trA1+trA2+trA3+trA4)/5\n",
    "\n",
    "# get trA for SGD_with_stumps_Classifier\n",
    "trA_for_sgd_with_stumps = get_trA_for_sgd_with_stumps(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4)\n",
    "print(trA_for_sgd_with_stumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the rest of your code here. Anything from here down should be commented\n",
    "#out when you submit to the autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the prediction for unlabeled txt using the final trained classifier.\n",
    "sgd = train_sgd(X_all, y_all)\n",
    "y_sgd = predict_clf(sgd, X_unlabeled)\n",
    "ouput_predictions(y_sgd, 'sgd.txt')\n",
    "\n",
    "dt = train_decision_tree(X_all, y_all)\n",
    "y_dt = predict_clf(dt, X_unlabeled)\n",
    "ouput_predictions(y_dt, 'dt.txt')\n",
    "\n",
    "dt_4 = train_decision_tree(X_all, y_all,4)\n",
    "y_dt_4 = predict_clf(dt_4, X_unlabeled)\n",
    "ouput_predictions(y_dt_4, 'dt-4.txt')\n",
    "\n",
    "dt_8 = train_decision_tree(X_all, y_all,8)\n",
    "y_dt_8 = predict_clf(dt_8, X_unlabeled)\n",
    "ouput_predictions(y_dt_8, 'dt-8.txt')\n",
    "\n",
    "res = train_sgd_with_stumps(X_all, y_all)\n",
    "sgd_with_stumps = res[0]\n",
    "stumps = res[1]\n",
    "y_sgd_dt = predict_sgd_with_stumps(sgd_with_stumps, stumps, X_unlabeled)\n",
    "ouput_predictions(y_sgd_dt, 'sgd-dt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81 1.0 0.6414285714285715 0.6914285714285714 0.96\n"
     ]
    }
   ],
   "source": [
    "# Calculate the trA\n",
    "\n",
    "y_sgd_predict = predict_clf(sgd, X_all)\n",
    "score_sgd = get_accurancy(y_sgd_predict,y_all)\n",
    "\n",
    "y_dt_predict = predict_clf(dt, X_all)\n",
    "score_dt = get_accurancy(y_dt_predict,y_all)\n",
    "\n",
    "y_dt4_predict = predict_clf(dt_4, X_all)\n",
    "score_dt4 = get_accurancy(y_dt4_predict,y_all)\n",
    "\n",
    "y_dt8_predict = predict_clf(dt_8, X_all)\n",
    "score_dt8 = get_accurancy(y_dt8_predict,y_all)\n",
    "\n",
    "\n",
    "y_sgd_dt_predict = predict_sgd_with_stumps(sgd_with_stumps, stumps, X_all)\n",
    "score_sgd_dt = get_accurancy(y_sgd_dt_predict,y_all)\n",
    "\n",
    "print(score_sgd,score_dt,score_dt4,score_dt8,score_sgd_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# When degrees of freedom of n-1 = 4 and 99% confidence, critical value t = 4.604\n",
    "def get_intervals_offset(A): \n",
    "    x_bar = np.mean(A)\n",
    "    S = np.std(A)\n",
    "    n = len(A)\n",
    "    return 4.604*S/np.square(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cal the Conf. interval for sgd:\n",
    "offset_sgd = get_intervals_offset(pAs_sgd)\n",
    "offset_dt = get_intervals_offset(pAs_for_dt)\n",
    "offset_dt4 = get_intervals_offset(pAs_for_dt4)\n",
    "offset_dt8 = get_intervals_offset(pAs_for_dt8)\n",
    "offset_sgd_with_stumps = get_intervals_offset(pAs_sgd_with_stumps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trA_sgd= 0.8699999999999999\n",
      "pA_sgd= 0.6485714285714286 ± 0.010437575507672624\n",
      "trA_for_dt= 1.0\n",
      "pA_for_dt= 0.5742857142857142 ± 0.002439756545900699\n",
      "trA_for_dt4= 0.6639285714285714\n",
      "pA_for_dt4= 0.5985714285714285 ± 0.0034703389161079156\n",
      "trA_for_dt8= 0.7414285714285714\n",
      "pA_for_dt8= 0.5642857142857143 ± 0.007441187704189401\n",
      "trA_for_sgd_with_stumps= 0.9657142857142856\n",
      "pA_for_sgd_with_stumps= 0.6557142857142857 ± 0.005787885714285711\n"
     ]
    }
   ],
   "source": [
    "# Output all the result\n",
    "print(\"trA_sgd=\",trA_sgd)\n",
    "print(\"pA_sgd=\", pA_sgd,'±', offset_sgd)\n",
    "print(\"trA_for_dt=\",trA_for_dt)\n",
    "print(\"pA_for_dt=\", pA_for_dt,'±', offset_dt)\n",
    "print(\"trA_for_dt4=\",trA_for_dt4)\n",
    "print(\"pA_for_dt4=\", pA_for_dt4,'±', offset_dt4)\n",
    "print(\"trA_for_dt8=\",trA_for_dt8)\n",
    "print(\"pA_for_dt8=\", pA_for_dt8,'±', offset_dt8)\n",
    "print(\"trA_for_sgd_with_stumps=\",trA_for_sgd_with_stumps)\n",
    "print(\"pA_for_sgd_with_stumps=\",pA_for_sgd_with_stumps,'±', offset_sgd_with_stumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 informotive features and its occurences: \n",
      "[[37, 83], [364, 75], [104, 33], [3, 18], [24, 15], [69, 11], [123, 9], [0, 7], [85, 6], [12, 5], [97, 4], [183, 3], [53, 3], [378, 3], [54, 3], [2, 3], [260, 3], [95, 2], [25, 2], [288, 2], [65, 2], [286, 2], [10, 1], [342, 1], [363, 1], [368, 1], [227, 1], [17, 1]]\n",
      "The chacracter and the place represented by the top 50 informative features: \n",
      "[[1, 'l'], [14, 'a'], [4, 'a'], [0, 'd'], [0, 'y'], [2, 'r'], [4, 't'], [0, 'a'], [3, 'h'], [0, 'm'], [3, 't'], [7, 'b'], [2, 'b'], [14, 'o'], [2, 'c']]\n"
     ]
    }
   ],
   "source": [
    "# #Experiments: find the most informative features.\n",
    "\n",
    "# Split the train.txt into 2 splits 300 times, and train 300 different decision trees\n",
    "# Extract the root node as the most informative features.\n",
    "features = []\n",
    "for i in range(300):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, shuffle = True, test_size = 0.5)\n",
    "    stump =train_decision_tree(X_train, y_train, 8)\n",
    "    feature = stump.tree_.feature\n",
    "    features.append(feature[0])# Extract the root node\n",
    "    \n",
    "#  Use a dictionary to count the occurences of each extracted features.  \n",
    "#  for example:\n",
    "#  cnt = {[37, 77], [364, 72], [104, 31]……} \n",
    "#  it means the No.37th features is the most frequent root node in 200 different decision tree,it appears 77 times.\n",
    "#  and the No.364th features is the second frequent root node in 200 different decision tree,it appears 72 times.\n",
    "\n",
    "cnt = {}\n",
    "for feature in features:\n",
    "    if feature not in cnt:\n",
    "        cnt[feature] = 0\n",
    "    cnt[feature] += 1\n",
    "\n",
    "# Convert the cnt dictionary to list and sorted it by the features' occurences.Chunk the top 50 features.\n",
    "\n",
    "lst = []\n",
    "for key in cnt:\n",
    "    tmp = [key, cnt[key]]\n",
    "    lst.append(tmp)\n",
    "lst = sorted(lst, key=lambda x: x[1], reverse=True)\n",
    "lst = lst[:50]\n",
    "print(\"Top 50 informotive features and its occurences: \")\n",
    "print(lst)\n",
    "\n",
    "# Find what the features represent in the names.\n",
    "# for example, if No. 37 features is 1, it means in the first name, the second character is 'l'.\n",
    "# 0 1 2 3 4 5     6 7 8 9 10      11 12 13 14 15\n",
    "# first name      middle name       last name\n",
    "\n",
    "al=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "feas = []\n",
    "for fea in lst[:15]:\n",
    "    tmp = [fea[0]//26, al[fea[0]%26]]\n",
    "    feas.append(tmp)\n",
    "print(\"The chacracter and the place represented by the top 50 informative features: \")\n",
    "print(feas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sgd_with_most_informotive_features(sgd, dict_features, X):\n",
    "\n",
    "    # Create new X new  using  above\n",
    "    X_new = []\n",
    "    for row in X:\n",
    "        tmp = []\n",
    "        for key,value in dict_features:\n",
    "            tmp.append(row[key])\n",
    "        X_new.append(tmp)\n",
    "    return predict_clf(sgd, X_new)\n",
    "\n",
    "\n",
    "def train_sgd_with_most_informotive_features(X, y, dict_features):\n",
    "    sgd_with_most_informotive_features = SGDClassifier(loss='log',alpha = 0.001, tol=0.2,learning_rate='optimal', penalty='l2')\n",
    "\n",
    "    # Create new X new  using  above\n",
    "    X_new = []\n",
    "    for row in X:\n",
    "        tmp = []\n",
    "        for key,value in dict_features:\n",
    "            \n",
    "            tmp.append(row[key])\n",
    "        X_new.append(tmp)\n",
    "\n",
    "    sgd_with_most_informotive_features = sgd_with_most_informotive_features.fit(X_new, y)\n",
    "    return sgd_with_most_informotive_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pA 99% Conf. Intervals =  0.6699999999999999 ± 0.006519007066999682\n"
     ]
    }
   ],
   "source": [
    "# get pA for SGD_with_stumps_Classifier:\n",
    "\n",
    "def get_pA_for_sgd_with_most_informotive_features (X0, X1, X2, X3, X4, y0, y1, y2, y3, y4,dict_features):\n",
    "    sgd_with_most_informotive_features0 = train_sgd_with_most_informotive_features(X1+X2+X3+X4, y1+y2+y3+y4,dict_features)\n",
    "    y_p0 = predict_sgd_with_most_informotive_features(sgd_with_most_informotive_features0, dict_features, X0)\n",
    "    pA0 = get_accurancy(y_p0, y0)\n",
    "\n",
    "    sgd_with_most_informotive_features1 = train_sgd_with_most_informotive_features(X1+X2+X3+X4, y1+y2+y3+y4,dict_features)\n",
    "    y_p1 = predict_sgd_with_most_informotive_features(sgd_with_most_informotive_features1, dict_features, X1)\n",
    "    pA1 = get_accurancy(y_p1, y1)\n",
    "\n",
    "    sgd_with_most_informotive_features2 = train_sgd_with_most_informotive_features(X1+X2+X3+X4, y1+y2+y3+y4,dict_features)\n",
    "    y_p2 = predict_sgd_with_most_informotive_features(sgd_with_most_informotive_features2, dict_features, X2)\n",
    "    pA2 = get_accurancy(y_p2, y2)\n",
    "\n",
    "\n",
    "    sgd_with_most_informotive_features3 = train_sgd_with_most_informotive_features(X1+X2+X3+X4, y1+y2+y3+y4,dict_features)\n",
    "    y_p3 = predict_sgd_with_most_informotive_features(sgd_with_most_informotive_features3, dict_features, X3)\n",
    "    pA3 = get_accurancy(y_p3, y3)\n",
    "    \n",
    "    sgd_with_most_informotive_features4 = train_sgd_with_most_informotive_features(X1+X2+X3+X4, y1+y2+y3+y4,dict_features)\n",
    "    y_p4 = predict_sgd_with_most_informotive_features(sgd_with_most_informotive_features4, dict_features, X4)\n",
    "    pA4 = get_accurancy(y_p4, y4)    \n",
    "    \n",
    "    pAs = [pA0,pA1,pA2,pA3,pA4]\n",
    "    return sum(pAs)/5, pAs\n",
    "\n",
    "# # get pA for SGD_with_stumps_Classifier\n",
    "tuples =  get_pA_for_sgd_with_most_informotive_features(X0, X1, X2, X3, X4, y0, y1, y2, y3, y4,lst)\n",
    "pA_for_sgd_with_stumps = tuples[0]\n",
    "pAs_sgd_with_stumps = tuples[1]\n",
    "offset_sgd_with_most_informotive_features = get_intervals_offset(pAs_sgd_with_stumps)\n",
    "\n",
    "print(\"pA 99% Conf. Intervals = \", pA_for_sgd_with_stumps,'±',offset_sgd_with_most_informotive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
